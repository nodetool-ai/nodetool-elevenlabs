# This file is auto-generated by nodetool.dsl.codegen.
# Please do not edit this file manually.

# Instead, edit the node class in the source module and run the following commands to regenerate the DSL:
# nodetool package scan
# nodetool codegen

from pydantic import BaseModel, Field
import typing
from typing import Any
import nodetool.metadata.types
import nodetool.metadata.types as types
from nodetool.dsl.graph import GraphNode, SingleOutputGraphNode

import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.elevenlabs.text_to_speech
from nodetool.workflows.base_node import BaseNode


class RealtimeTextToSpeech(
    GraphNode[nodetool.nodes.elevenlabs.text_to_speech.RealtimeTextToSpeech.OutputType]
):
    """

    Stream text-to-speech using ElevenLabs WebSocket API. Consumes text chunks and outputs audio chunks in real-time.
    audio, tts, speech, streaming, realtime, websocket

    Use cases:
    - Real-time voice generation from streaming text
    - Interactive voice applications
    - Low-latency text-to-speech conversion
    - Streaming dialogue generation
    """

    VoiceIDEnum: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum
    )
    ModelID: typing.ClassVar[type] = nodetool.nodes.elevenlabs.text_to_speech.ModelID
    LanguageID: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.LanguageID
    )
    OutputFormat: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.OutputFormat
    )

    voice: nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum.ARIA,
        description="Voice ID to be used for generation",
    )
    chunk: types.Chunk | OutputHandle[types.Chunk] = connect_field(
        default=types.Chunk(
            type="chunk",
            node_id=None,
            thread_id=None,
            workflow_id=None,
            content_type="text",
            content="",
            content_metadata={},
            done=False,
            thinking=False,
        ),
        description="The text chunk to use as input.",
    )
    model_id: nodetool.nodes.elevenlabs.text_to_speech.ModelID = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.ModelID.TURBO_V2_5,
        description="The TTS model to use for generation",
    )
    language_code: nodetool.nodes.elevenlabs.text_to_speech.LanguageID = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.LanguageID.NONE,
        description="Language code to enforce (only works with Turbo v2.5)",
    )
    output_format: nodetool.nodes.elevenlabs.text_to_speech.OutputFormat = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.OutputFormat.MP3_44100_128,
        description="Audio output format for streaming",
    )
    stability: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Voice stability (0-1). Higher values make output more consistent",
    )
    similarity_boost: float | OutputHandle[float] = connect_field(
        default=0.75, description="Similarity to original voice (0-1)"
    )
    style: float | OutputHandle[float] = connect_field(
        default=0.0, description="Speaking style emphasis (0-1)"
    )
    use_speaker_boost: bool | OutputHandle[bool] = connect_field(
        default=True, description="Whether to use speaker boost for clearer output"
    )
    speed: float | OutputHandle[float] = connect_field(
        default=1.0, description="Speed of the generated speech (0.7-1.2)"
    )
    enable_ssml_parsing: bool | OutputHandle[bool] = connect_field(
        default=False, description="Enable SSML parsing in text input"
    )

    @property
    def out(self) -> "RealtimeTextToSpeechOutputs":
        return RealtimeTextToSpeechOutputs(self)

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.elevenlabs.text_to_speech.RealtimeTextToSpeech

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()


class RealtimeTextToSpeechOutputs(OutputsProxy):
    @property
    def chunk(self) -> OutputHandle[types.Chunk]:
        return typing.cast(OutputHandle[types.Chunk], self["chunk"])


import typing
from pydantic import Field
from nodetool.dsl.handles import OutputHandle, OutputsProxy, connect_field
import nodetool.nodes.elevenlabs.text_to_speech
from nodetool.workflows.base_node import BaseNode


class TextToSpeech(SingleOutputGraphNode[types.AudioRef], GraphNode[types.AudioRef]):
    """

    Generate natural-sounding speech using ElevenLabs' advanced text-to-speech technology. Features multiple voices and customizable parameters.
    audio, tts, speech, synthesis, voice

    Use cases:
    - Create professional voiceovers
    - Generate character voices
    - Produce multilingual content
    - Create audiobooks
    - Generate voice content
    """

    VoiceIDEnum: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum
    )
    ModelID: typing.ClassVar[type] = nodetool.nodes.elevenlabs.text_to_speech.ModelID
    LanguageID: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.LanguageID
    )
    TextNormalization: typing.ClassVar[type] = (
        nodetool.nodes.elevenlabs.text_to_speech.TextNormalization
    )

    voice: nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.VoiceIDEnum.ARIA,
        description="Voice ID to be used for generation",
    )
    text: str | OutputHandle[str] = connect_field(
        default="Hello, how are you?", description="The text to convert to speech"
    )
    tts_model_id: nodetool.nodes.elevenlabs.text_to_speech.ModelID = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.ModelID.MONOLINGUAL_V1,
        description="The TTS model to use for generation",
    )
    voice_settings: dict | OutputHandle[dict] = connect_field(
        default={}, description="Optional voice settings to override defaults"
    )
    language_code: nodetool.nodes.elevenlabs.text_to_speech.LanguageID = Field(
        default=nodetool.nodes.elevenlabs.text_to_speech.LanguageID.NONE,
        description="Language code to enforce (only works with Turbo v2.5)",
    )
    optimize_streaming_latency: int | OutputHandle[int] = connect_field(
        default=2,
        description="Latency optimization level (0-4). Higher values trade quality for speed",
    )
    seed: int | OutputHandle[int] = connect_field(
        default=-1,
        description="Seed for deterministic generation (0-4294967295). -1 means random",
    )
    text_normalization: nodetool.nodes.elevenlabs.text_to_speech.TextNormalization = (
        Field(
            default=nodetool.nodes.elevenlabs.text_to_speech.TextNormalization.AUTO,
            description="Controls text normalization behavior",
        )
    )
    stability: float | OutputHandle[float] = connect_field(
        default=0.5,
        description="Voice stability (0-1). Higher values make output more consistent, lower values more varied",
    )
    similarity_boost: float | OutputHandle[float] = connect_field(
        default=0.75,
        description="Similarity to original voice (0-1). Higher values make output closer to original voice",
    )
    style: float | OutputHandle[float] = connect_field(
        default=0.0,
        description="Speaking style emphasis (0-1). Higher values increase style expression",
    )
    use_speaker_boost: bool | OutputHandle[bool] = connect_field(
        default=False,
        description="Whether to use speaker boost for clearer, more consistent output",
    )

    @classmethod
    def get_node_class(cls) -> type[BaseNode]:
        return nodetool.nodes.elevenlabs.text_to_speech.TextToSpeech

    @classmethod
    def get_node_type(cls):
        return cls.get_node_class().get_node_type()
